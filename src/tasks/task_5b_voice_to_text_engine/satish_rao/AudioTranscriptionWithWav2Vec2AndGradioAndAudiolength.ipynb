{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub"
      ],
      "metadata": {
        "id": "zVVXJy1kc1CF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "xA8XDwXVc96P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install gradio -q\n"
      ],
      "metadata": {
        "id": "EnpgfeAADS59"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "HkHIAUT7cI28"
      },
      "outputs": [],
      "source": [
        "import soundfile as sf\n",
        "import librosa\n",
        "from pydub import AudioSegment\n",
        "import torch\n",
        "\n",
        "from IPython.display import Audio\n",
        "from scipy.io import wavfile\n",
        "import numpy as np\n",
        "\n",
        "import gradio as gr\n",
        "import time\n",
        "\n",
        "from transformers import Wav2Vec2ForCTC, Wav2Vec2Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#load pre-trained model and tokenizer\n",
        "tokenizer = Wav2Vec2Tokenizer.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
        "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")"
      ],
      "metadata": {
        "id": "nCzAuyyexjbQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transcribe(audio, length):\n",
        "\n",
        "  Audio(audio)\n",
        "\n",
        "  data = wavfile.read(audio)\n",
        "  framerate = data[0]\n",
        "  sounddata = data[1]\n",
        "  time = np.arange(0,len(sounddata))/framerate\n",
        "  print(framerate)\n",
        "  total_time = len(sounddata)/framerate\n",
        "  print('Input length:', length)\n",
        "  print('Total time:', len(sounddata)/framerate)\n",
        "\n",
        "  if (total_time > length):\n",
        "      return \"Input audio is longer than suggested length\"\n",
        "\n",
        "  input_audio,_ = librosa.load(audio, sr=16000)\n",
        "\n",
        "  input_values = tokenizer(input_audio, return_tensors=\"pt\").input_values\n",
        "  logits = model(input_values).logits\n",
        "  predicted_ids = torch.argmax(logits, dim=-1)\n",
        "  text = tokenizer.batch_decode(predicted_ids)[0]\n",
        "\n",
        "  print(text)\n",
        "\n",
        "  return text"
      ],
      "metadata": {
        "id": "fRt-d1Bpsgv5"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr.Interface(\n",
        "    title = 'Real-time AI-based Audio Transcription, Recognition and Translation Web App',\n",
        "    fn=transcribe,\n",
        "    inputs=[\n",
        "        gr.inputs.Audio(source=\"microphone\", type=\"filepath\"),\n",
        "        gr.Number(value=60)\n",
        "    ],\n",
        "    outputs=[\n",
        "        \"textbox\"\n",
        "    ],\n",
        "    live=True).launch()"
      ],
      "metadata": {
        "id": "sw0C-8Zysk39"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}