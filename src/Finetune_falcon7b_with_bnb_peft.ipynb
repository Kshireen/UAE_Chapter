{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "11wFGVxPsmKm"
      },
      "source": [
        "Ref: https://www.youtube.com/watch?v=DcBC4yGHV4Q"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE5GJ6s7y0Xo"
      },
      "source": [
        "### Fine-tune large models using ðŸ¤— [`peft`](https://github.com/huggingface/peft) adapters, [`transformers`](https://github.com/huggingface/transformers) & [`bitsandbytes`](https://github.com/TimDettmers/bitsandbytes)\n",
        "\n",
        "I am doing my project experiment  wiht fine-tune large language models using the very recent `peft` library and `bitsandbytes` for loading large models in **8-bit**.\n",
        "The fine-tuning method relies on a recent method called \"Low Rank Adapters\" ([LoRA](https://arxiv.org/pdf/2106.09685.pdf)), instead of fine-tuning the entire model, we just have to fine-tune these adapters and load them properly inside the model.\n",
        "After fine-tuning the model, I am goinf to share the model adapters on the ðŸ¤— Hub and load them very easily.\n",
        "Let me start!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfBzP8gWzkpv"
      },
      "source": [
        "### Install requirements\n",
        "\n",
        "First, running the cells below to install the requirements:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "otj46qRbtpnd"
      },
      "outputs": [],
      "source": [
        "!pip install -q bitsandbytes datasets accelerate loralib einops\n",
        "!pip install -q git+https://github.com/huggingface/transformers.git@main git+https://github.com/huggingface/peft.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDz79Y4iWzUv"
      },
      "source": [
        "## Checking Graphic Cards presence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "I2-xmZCnVDg_"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kulAjt0lWzUv"
      },
      "source": [
        "## Importing Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "koRYhzEOSyb0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import bitsandbytes as bnb\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModelForCausalLM,\n",
        "    BitsAndBytesConfig,\n",
        ")\n",
        "\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PeftConfig,\n",
        "    PeftModel,\n",
        "    get_peft_model,\n",
        "    prepare_model_for_kbit_training\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOtwYRI3zzXI"
      },
      "source": [
        "## Huggingface Credentials  + Google Drive Mounting (later reqquired for dataset loading)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1rvb_IqLrmRo"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6bwCKOvJWzUx"
      },
      "source": [
        "## Initializing Parameters / Settings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8vHtolPVZJj"
      },
      "outputs": [],
      "source": [
        "free_in_GB = int(torch.cuda.mem_get_info()[0] / 1024**3)\n",
        "max_memory = f\"{free_in_GB-2}GB\"\n",
        "\n",
        "n_gpus = torch.cuda.device_count()\n",
        "max_memory = {i: max_memory for i in range(n_gpus)}\n",
        "max_memory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HjV7-v11WzU3"
      },
      "source": [
        "## Loading Pre-trained Model & Tokenizer from repositoiry"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wFCmbcdmGT_O"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"tiiuae/falcon-7b\"   # original \"tiiuae/falcon-7b\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JemR-9xa8D6k"
      },
      "outputs": [],
      "source": [
        "from transformers.modeling_utils import BitsAndBytesConfig\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit = True,   # can be 4bit / 8bit\n",
        "    #load_in_8bit = True,   # can be 4bit / 8bit\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type='nf4',    # minmax, meanmin\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        "    #bnb_8bit_compute_dtype=torch.bfloat16\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvAuF0KO3UcG"
      },
      "outputs": [],
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map='auto',\n",
        "    trust_remote_code=True,\n",
        "    quantization_config = bnb_config\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9fTSZntA1iUG"
      },
      "source": [
        "## Prepare model for training\n",
        "\n",
        "Some pre-processing will be done before training such an int8 model using `peft`, therefore I will import an utiliy function `prepare_model_for_kbit_training` that will:\n",
        "- Casts all the non `int8` modules to full precision (`fp32`) for stability\n",
        "- Add a `forward_hook` to the input embedding layer to enable gradient computation of the input hidden states\n",
        "- Enable gradient checkpointing for more memory-efficient training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-gy-LxM0yAi"
      },
      "outputs": [],
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KwOTr7B3NlM3"
      },
      "source": [
        "### Apply LoRA\n",
        "\n",
        "Here I will utilize the magical 'parameter efficient fine tuning' `peft`!, that is loading a `PeftModel` and specify to use low-rank adapters (LoRA) using `get_peft_model` utility function from `peft`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4iwHGzKBN6wk"
      },
      "outputs": [],
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"query_key_value\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwa3J-cMD57T"
      },
      "source": [
        "### Model Architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LeeJ_5BbDF02"
      },
      "outputs": [],
      "source": [
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxO1Q0KxWzU-"
      },
      "source": [
        "### Printing trainable parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hZ1u1aqmWzU_"
      },
      "outputs": [],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcnYwJRXWzU_"
      },
      "source": [
        "### Data loading from prepared json file"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qzuMRl6NWzVA"
      },
      "source": [
        "### prompt generation configuration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2s-cxHMoAQM7"
      },
      "outputs": [],
      "source": [
        "generation_config = model.generation_config\n",
        "\n",
        "generation_config.max_new_tokens = 200\n",
        "generation_config.temperature = 0.7\n",
        "generation_config.do_sample = False # new\n",
        "generation_config.top_p = 0.7\n",
        "generation_config.top_k = 20\n",
        "generation_config.num_return_sequences = 1\n",
        "generation_config.pad_token_id = tokenizer.eos_token_id\n",
        "generation_config.eos_token_id = tokenizer.eos_token_id\n",
        "generation_config.use_cache = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nP69C0OBE63"
      },
      "outputs": [],
      "source": [
        "generation_config"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bw_7EHBu_IuD"
      },
      "source": [
        "### Inference Before Training\n",
        "This is just to check the model is loaded properly, and the actual inference shall be done in a separate module namd with 'My project -Inference'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mTbNFxWk_Lyc"
      },
      "outputs": [],
      "source": [
        "prompt = f\"\"\"\n",
        "<bot>: How can I wash my hand?\n",
        "<human>:\n",
        "\"\"\".strip()\n",
        "print(prompt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zobr-Y1h_haG"
      },
      "outputs": [],
      "source": [
        "%%script true\n",
        "\n",
        "%%time\n",
        "device = \"cuda:0\"\n",
        "\n",
        "encoding = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "with torch.inference_mode(True):\n",
        "    outputs = model.generate(\n",
        "        input_ids=encoding.input_ids,\n",
        "        attention_mask=encoding.attention_mask,\n",
        "        generation_config=generation_config,\n",
        "    )\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YEB6qMmAqSU1"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, Dataset\n",
        "json_file_path = '/content/drive/MyDrive/ISP/data/final_qa.json'\n",
        "\n",
        "# loading saved json data\n",
        "data = load_dataset('json', data_files = json_file_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMQG-X1rnPFC"
      },
      "outputs": [],
      "source": [
        "print('question: ',data['train'][50]['question'])\n",
        "print('answer: ', data['train'][50]['answer'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mh31fiChVgUz"
      },
      "outputs": [],
      "source": [
        "print(len(data['train']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdjWif4CVXR6"
      },
      "source": [
        "## Build HuggingFace dataset / Model compatible"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJeSAxFjD65c"
      },
      "outputs": [],
      "source": [
        "# This generates data as per already defined prompt titles (human/Assistant)\n",
        "\n",
        "def generate_prompt(data_point):\n",
        "    return f\"\"\"\n",
        ": {data_point[\"question\"]}\n",
        ": {data_point[\"answer\"]}\n",
        "\"\"\".strip()\n",
        "\n",
        "def generate_and_tokenize_prompt(data_point):\n",
        "    full_prompt = generate_prompt(data_point)\n",
        "    tokenized_full_prompt = tokenizer(full_prompt, padding=True, truncation=True)\n",
        "    return tokenized_full_prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUJ1iILTGQrT"
      },
      "outputs": [],
      "source": [
        "# generate train data in the form of tokenized prompts\n",
        "train_data_enc = data[\"train\"].shuffle().map(generate_and_tokenize_prompt)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "InpuNBeCx3hP"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "87Fach8Kjhh9"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vAEpbe3aHQDY"
      },
      "outputs": [],
      "source": [
        "OUTPUT_DIR = '/content/drive/MyDrive/ISP/gc1-Falcon/experiments'\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir '/content/drive/MyDrive/ISP/gc1-Falcon/experiments'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AQ_HCYruWIHU"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "import transformers\n",
        "\n",
        "training_args = transformers.TrainingArguments(\n",
        "    per_device_train_batch_size = 32, # adjust as per vram of GPU\n",
        "    auto_find_batch_size=True,\n",
        "    gradient_accumulation_steps=4,\n",
        "    num_train_epochs=1,\n",
        "    max_steps=120,\n",
        "    learning_rate=1e-4,\n",
        "    fp16=True,\n",
        "    save_strategy= 'epoch',\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "    lr_scheduler_type = 'cosine',\n",
        "    warmup_ratio = 0.05,\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    logging_steps=1,\n",
        "    report_to = 'tensorboard',\n",
        "    save_total_limit=3,\n",
        ")\n",
        "\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=train_data_enc,\n",
        "    args=training_args,\n",
        "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
        ")\n",
        "\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "\n",
        "# training loop\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Duak7T_B3VpJ"
      },
      "source": [
        "## Finetuned model adapter :  Save to Disk and Share on the ðŸ¤— Hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W3uSITlrRBo_"
      },
      "outputs": [],
      "source": [
        "FINETUNED_MODEL_NAME = 'TariqJamil/falcon-7b-peft-qlora-my_finetuned_model-0706'\n",
        "\n",
        "model.save_pretrained(FINETUNED_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VxB6UV5XAvvP"
      },
      "outputs": [],
      "source": [
        "#model.push_to_hub('falcon-7b-instruct-peft-qlora-my_finetuned_model-0607', use_auth_token=True, create_pr=1)\n",
        "model.push_to_hub(FINETUNED_MODEL_NAME, private=True)\n",
        "tokenizer.push_to_hub(FINETUNED_MODEL_NAME)\n",
        "model.config.to_json_file(\"config.json\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXy_Murc9Ltv"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.17"
    },
    "vscode": {
      "interpreter": {
        "hash": "6c4e21ff5edce2fb2cfe7eb854551da92c6ec05cac2504057bb7aba62f43a5ec"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}